> [[初始文档]]

## 信任边界：GIGO定律与放大器效应

RAG 技术解决了 AI “瞎编”的智力幻觉，但引入了新的风险：**忠实地重复错误**。

### 1. 放大器效应
- **Garbage In, Garbage Out**：如果你给 NotebookLM 喂食错误的信息（如“地球是方的”），它会基于 Grounding 机制，言之凿凿地告诉你“地球是方的”。
- **引用误导**：它附上的引用 `[1]` 会给你一种视觉上的“权威感”，让你误以为这是经过验证的真理，实则只是经过索引的谬误。

### 2. 构建要求：入库审查
既然 AI 把判别真伪的责任甩给了用户，构建知识库就必须有“洁癖”：
- **对抗性阅读**：不要只上传单一来源。同时上传正反两方观点，利用 NotebookLM 的对比分析能力来发现矛盾。
- **动态清理**：及时剔除过时资料，防止 AI 引用“过期真理”。

**结论**：RAG 是一个逻辑放大器。给它金子，它还你金矿；给它垃圾，它还你一座逻辑严密的垃圾山。

> [[知识卡片/05. RAG消除了幻觉，但如果我给的资料本身就是错的呢？.md]]